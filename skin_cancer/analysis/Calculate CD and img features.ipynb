{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch.utils.data as utils\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import pickle as pkl\n",
    "\n",
    "from os.path import join as oj\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import seaborn as sns\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.utils.data as utils\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "sys.path.append(\"../../fit\")\n",
    "from tqdm import tqdm_notebook\n",
    "import cd\n",
    "from shutil import copyfile\n",
    "from os.path import join as oj\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "from torch import nn    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../../../pytorch-cnn-visualizations/src\")\n",
    "from gradcam import GradCam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.asarray([0.485, 0.456, 0.406])\n",
    "std = np.asarray([0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"../../results_for_export\"\n",
    "device = torch.device(\"cuda\")\n",
    "import torchvision.models as models\n",
    "\n",
    "model = models.vgg16(pretrained=True).to(device)\n",
    "model.classifier[-1] = nn.Linear(4096, 2)\n",
    "model.load_state_dict(torch.load('../models/81407832282747730878.pt'))\n",
    "model = model.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the two imgs\n",
    "data_path = \"../../../../datasets\"\n",
    "img_path = oj(data_path, \"ISIC/not_cancer\")\n",
    "seg_path  = oj(data_path, \"segmentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check that it learned the bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(oj(img_path, \"ISIC_0000603.jpg\"))\n",
    "img_np = np.asarray(img)/255.0\n",
    "img.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg = Image.open(oj(seg_path, \"ISIC_0000603.jpg\"))\n",
    "seg_np = np.asarray(seg)[:,:,0]\n",
    "seg_np = seg_np > seg_np.mean()\n",
    "seg.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img_np * (1- seg_np).astype(np.float32)[:,:,None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not super conclusive but "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_img = torch.from_numpy(((img_np - mean)/std).swapaxes(0,2).swapaxes(1,2)).cuda().float()\n",
    "model(torch_img[None, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = cd.cd_track_vgg(seg_np[None, :], torch_img[None, :], model)\n",
    "print(\"Relevant:\")\n",
    "print(out[0])\n",
    "print(\"Irrelevant:\")\n",
    "print(out[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "not_cancer_cd = torch.nn.functional.softmax(torch.abs(torch.cat((out[0][0][1][None,], out[1][0][1][None,]),dim=0).data))\n",
    "cancer_cd = torch.nn.functional.softmax(torch.abs(torch.cat((out[0][0][0][None,], out[1][0][0][None,]),dim=0).data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gradCAM = GradCam(model,30)\n",
    "\n",
    "fig, axes = plt.subplots(ncols =3)\n",
    "axes[0].imshow(img_np[50:274, 50:274])\n",
    "test_out = model_gradCAM.generate_cam(torch_img[None, :,50:274, 50:274].cpu(), target_class= 0)\n",
    "\n",
    "axes[1].imshow(test_out)\n",
    "test_out = model_gradCAM.generate_cam(torch_img[None, :,50:274, 50:274].cpu(), target_class= 1)\n",
    "\n",
    "axes[2].imshow(test_out)\n",
    "# GradCAM agrees that this is good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features, extract "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract features and marry them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../../../../datasets\"\n",
    "save_path = oj(data_path, \"ISIC_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "import torchvision.models as models\n",
    "from torch.nn import AdaptiveAvgPool2d\n",
    "model = models.vgg16(pretrained=True).to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the two imgs\n",
    "data_path = \"../../../../datasets\"\n",
    "img_path = oj(data_path, \"ISIC/not_cancer\")\n",
    "seg_path  = oj(data_path, \"segmentation\")\n",
    "list_of_img_names = os.listdir(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.asarray([0.485, 0.456, 0.406])\n",
    "std = np.asarray([0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_features = np.empty((len(list_of_img_names), 25088))\n",
    "cd_features = -np.ones((len(list_of_img_names), 2, 25088)) # rel, irrel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_layer = torch.nn.AdaptiveAvgPool2d((7,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9e038f8346f44f0af34e026c9ab4a9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=19372), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for i in tqdm_notebook(range(len(list_of_img_names))):\n",
    "        img = Image.open(oj(img_path, list_of_img_names[i]))\n",
    "        img_torch = torch.from_numpy(((np.asarray(img)/255.0 -mean)/std).swapaxes(0,2).swapaxes(1,2))[None,:].cuda().float()\n",
    "        img.close()\n",
    "        img_features[i] = avg_layer(model.features(img_torch)).view(-1).cpu().numpy()\n",
    "        if os.path.isfile(oj(seg_path, list_of_img_names[i])):\n",
    "            seg = Image.open(oj(seg_path, list_of_img_names[i]))\n",
    "            blob = (np.asarray(seg)[:,:, 0] > 100).astype(np.float32)\n",
    "            rel, irrel =cd.cd_vgg_features(blob, img_torch, model)\n",
    "            cd_features[i, 0] = rel[0].cpu().numpy()\n",
    "            cd_features[i, 1] = irrel[0].cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(oj(save_path, \"not_cancer.npy\"), 'wb') as f:\n",
    "    np.save(f, img_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(oj(save_path, \"not_cancer_cd.npy\"), 'wb') as f:\n",
    "    np.save(f, cd_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now do the same for all the cancer images\n",
    "# potentially there will be some ill effect "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "085dfd81d0cd48f5ac1f4dba3d590b22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2282), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_path = \"../../../../datasets\"\n",
    "img_path = oj(data_path, \"ISIC/cancer\")\n",
    "list_of_img_names = os.listdir(img_path)\n",
    "img_features = np.empty((len(list_of_img_names), 25088))\n",
    "with torch.no_grad():\n",
    "    for i in tqdm_notebook(range(len(list_of_img_names))):\n",
    "        img = Image.open(oj(img_path, list_of_img_names[i]))\n",
    "        img_torch = torch.from_numpy(((np.asarray(img)/255.0 -mean)/std).swapaxes(0,2).swapaxes(1,2))[None,:].cuda().float()\n",
    "        img.close()\n",
    "        img_features[i] = avg_layer(model.features(img_torch)).view(-1).cpu().numpy()\n",
    "with open(oj(save_path, \"cancer.npy\"), 'wb') as f:\n",
    "    np.save(f, img_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../../../../datasets\"\n",
    "save_path = oj(data_path, \"ISIC_features\")\n",
    "from torch.utils.data import TensorDataset, ConcatDataset\n",
    "with open(oj(save_path, \"cancer.npy\"), 'rb') as f:\n",
    "    cancer_featuress = np.load(f)\n",
    "with open(oj(save_path, \"not_cancer.npy\"), 'rb') as f:\n",
    "    not_cancer_featuress = np.load(f)\n",
    "    \n",
    "cancer_targets = np.ones((cancer_featuress.shape[0])).astype(np.int64)\n",
    "not_cancer_targets = np.zeros((not_cancer_featuress.shape[0])).astype(np.int64)\n",
    "with open(oj(save_path, \"not_cancer_cd.npy\"), 'rb') as f:\n",
    "    not_cancer_cd= np.load(f)\n",
    "not_cancer_dataset = TensorDataset(torch.from_numpy(not_cancer_featuress).float(), torch.from_numpy(not_cancer_targets),torch.from_numpy(not_cancer_cd).float())\n",
    "\n",
    "cancer_dataset = TensorDataset(torch.from_numpy(cancer_featuress).float(), torch.from_numpy(cancer_targets),torch.from_numpy(-np.ones((len(cancer_featuress), 2, 25088))).float())\n",
    "complete_dataset = ConcatDataset((cancer_dataset, not_cancer_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.vgg16(pretrained=True)\n",
    "# make conv untrainable - test if needed\n",
    "model.classifier[-1] = nn.Linear(4096, 2)\n",
    "model = model.classifier.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_total = len(complete_dataset)\n",
    "num_train = int(0.8 * num_total)\n",
    "num_val = int(0.1 * num_total)\n",
    "num_test = num_total - num_train - num_val\n",
    "torch.manual_seed(0);\n",
    "train_dataset, test_dataset, val_dataset= torch.utils.data.random_split(complete_dataset, [num_train, num_test, num_val])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {'train' : train_dataset, 'test':test_dataset, 'val': val_dataset}\n",
    "dataset_sizes = {'train' : len(train_dataset), 'test':len(test_dataset), 'val': len(val_dataset)}\n",
    "torch.manual_seed(0)\n",
    "\n",
    "dataloaders = {x: torch.utils.data.DataLoader(datasets[x], batch_size=16,\n",
    "                                             shuffle=True, num_workers=4)\n",
    "              for x in ['train', 'test','val']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "136it [00:01, 88.76it/s]\n"
     ]
    }
   ],
   "source": [
    "for i, (inputs, labels, cd_features) in tqdm(enumerate(dataloaders['val'])):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask  = (cd_features[:, 0,0] != -1).byte().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel, irrel = cd.cd_vgg_features(cd_features[:,0].cuda(), cd_features[:,1].cuda(), inputs.cuda(), model)\n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel.masked_select()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save file, later load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all the images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all the segm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm_notebook(range(len(val_dataset))):\n",
    "    img = val_dataset[i][0].cuda()[None, :]\n",
    "    cd.cd_vgg_features(blob, img, model)[0];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gpu_usage)",
   "language": "python",
   "name": "gpu_usage"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
