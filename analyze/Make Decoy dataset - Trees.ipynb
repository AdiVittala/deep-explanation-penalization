{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make decoy datasets \n",
    "* input comprised of two sentences with agreeing sentiments in training but disagreeing in test (\"This movie is very good. I like it a lot\" vs \"This movie is very good. The director must have been brain-dead\")\n",
    "* Have random agreeing words in train but disagree in test\n",
    "* Key idea: structured noise \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "from os.path import join as oj\n",
    "import sys, time\n",
    "\n",
    "import csv\n",
    "from random import shuffle\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "from os.path import join\n",
    "import torch\n",
    "import torch\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from copy import deepcopy\n",
    "from model import LSTMSentiment\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import isdir\n",
    "\n",
    "from torchtext.data import TabularDataset\n",
    "# check out how two models differ\n",
    "import torch.optim as O\n",
    "import torch.nn as nn\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from tqdm import tqdm_notebook, tqdm \n",
    "import pickle\n",
    "from nltk import Tree\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cd import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataset(name):\n",
    "    \n",
    "    \n",
    "    data_path = \"./.data/sst/trees\"\n",
    "    os.makedirs(data_path exist_ok=True)\n",
    "    with open(oj(data_path, name + \".txt\")) as f:\n",
    "        content = []\n",
    "        content = f.read().splitlines()\n",
    "        \n",
    "    parsed_dataset = []\n",
    "    for line in (content):\n",
    "        t = Tree.fromstring(line)\n",
    "        text = t.leaves()\n",
    "        label = int(t.label()) \n",
    "\n",
    "        if label !=2: \n",
    "            label = int(label >2)\n",
    "            segment_labels = np.asarray([int(child.label()) for child in t])\n",
    "\n",
    "\n",
    "            if label ==1:\n",
    "                segment_labels = 4-segment_labels\n",
    "            diff = np.abs(segment_labels[0] - segment_labels[1])\n",
    " \n",
    "            segment_labels = (segment_labels >2).astype(np.float32)\n",
    "            if segment_labels.sum() ==0 or diff <2:\n",
    "                segment_labels = np.asarray([0.5, 0.5])\n",
    "            else:\n",
    "                segment_labels  =np.round(segment_labels/segment_labels.sum(), decimals =1)\n",
    "\n",
    "            stop_position = [int(len(child.leaves())) for child in t][0]\n",
    "            parsed_dataset.append((\" \".join(text), label, segment_labels[0],segment_labels[1] , stop_position))\n",
    "\n",
    "    file_path = \"../data\"\n",
    "    file_name = name + '_segment_annotated_SST.csv'\n",
    "    print(len(parsed_dataset))\n",
    "    with open(os.path.join(file_path, file_name), 'w') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        for line in parsed_dataset:\n",
    "\n",
    "            writer.writerow(line)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6920\n",
      "872\n",
      "1821\n"
     ]
    }
   ],
   "source": [
    "write_dataset(\"train\")\n",
    "write_dataset(\"dev\")\n",
    "write_dataset(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = data.Field(lower=True)\n",
    "answers = data.Field(sequential=False, unk_token=None)\n",
    "segment1_label = data.Field(sequential=False, unk_token=None, dtype=torch.float16)\n",
    "segment2_label = data.Field(sequential=False, unk_token=None, dtype=torch.float16)\n",
    "stop_pos = data.Field(sequential=False, unk_token=None)\n",
    "\n",
    "tv_datafields = [ (\"text\", inputs), \n",
    "                 (\"label\", answers), \n",
    "                 (\"segment1_label\", segment1_label), \n",
    "                 (\"segment2_label\", segment2_label), \n",
    "                 (\"stop_pos\", stop_pos)]\n",
    "train, dev, test = TabularDataset.splits(\n",
    "                           path=dataset_path,\n",
    "                           train='train_segment_annotated_SST.csv', validation=\"dev_segment_annotated_SST.csv\", test = \"test_segment_annotated_SST.csv\",\n",
    "                           format='csv', \n",
    "                           skip_header=False, # if your csv header has a header, make sure to pass this to ensure it doesn't get proceesed as data!\n",
    "                           fields=tv_datafields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random testing below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9565028901734104\n"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "test_list = []\n",
    "for i in range(len(train)):\n",
    "    test_list.append(float(train[i].segment2_label))\n",
    "    a+= float(train[i].segment2_label) ==0.5\n",
    "print(a/len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0, 0.5, 1.0}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gpu_usage)",
   "language": "python",
   "name": "gpu_usage"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
