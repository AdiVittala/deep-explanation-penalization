{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook, tqdm\n",
    "import pandas as pd\n",
    "from os.path import join as oj\n",
    "import torch\n",
    "import sys, time\n",
    "sys.path.insert(1, oj(sys.path[0], '..'))  # insert parent path\n",
    "sys.path.append('../fit')\n",
    "from cd import cd_batch_text, softmax_out, cd_penalty_for_one\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import pandas\n",
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "from os.path import join as oj\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "import sys, time\n",
    "sys.path.insert(1, oj(sys.path[0], '..'))  # insert parent path\n",
    "sys.path.append('../fit')\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "from cd import cd_batch_text, softmax_out, cd_penalty_for_one\n",
    "\n",
    "\n",
    "# In[24]:\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchtext import data\n",
    "import torch.optim as O\n",
    "torch.manual_seed(42)\n",
    "from model import LSTMSentiment\n",
    "\n",
    "\n",
    "from torchtext.data import Field\n",
    "import os\n",
    "import pandas\n",
    "import csv\n",
    "from torchtext.data import TabularDataset\n",
    "import torchtext\n",
    "import numpy as np\n",
    "#np.random.seed(42)\n",
    "import torch\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "\n",
    "# # Make dataset\n",
    "# \n",
    "\n",
    "# In[202]:\n",
    "\n",
    "repeats = 10 \n",
    "num_epochs = 50\n",
    "loss_weight_list = [1,]#0, 0.01, 0.1]#, 1, 2, 10]\n",
    "num_in_train = 20\n",
    "this_batch_size = 5\n",
    "\n",
    "char_class = ['a', 'b']\n",
    "char_noise = ['c', 'd', 'e', 'f']\n",
    "num_data =100\n",
    "dataset_path = \"../data\"\n",
    "max_length = 9\n",
    "noise = 0.0\n",
    "\n",
    "# In[203]:\n",
    "\n",
    "\n",
    "def make_dataset(num, noise = 0.0):\n",
    "    dataset_list=[]\n",
    "    has_noise = np.random.uniform(size = num) < noise\n",
    "    for i in range(num):\n",
    "        \n",
    "        my_output =np.random.randint(2)\n",
    "        my_input = [char_noise[np.random.randint(len(char_noise))] for x in range(max_length)] \n",
    "        if has_noise[i]:\n",
    "            my_input[np.random.randint(max_length)] = char_class[1-my_output]\n",
    "            my_input[np.random.randint(max_length)] = char_class[my_output]\n",
    "            \n",
    "        my_input[np.random.randint(max_length)] = char_class[my_output]\n",
    "\n",
    "        dataset_list.append([' '.join(my_input),  my_output ])\n",
    "    return dataset_list\n",
    "def write_dataset(file_path, file_name,num,add_noise =0):\n",
    "    my_dataset = make_dataset(num)\n",
    "    with open(os.path.join(file_path, file_name), 'w') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        for line in my_dataset:\n",
    "            writer.writerow(line)\n",
    "\n",
    "\n",
    "# In[204]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_device =0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "tokenize = lambda x: x.split()\n",
    "TEXT = Field(sequential=True,tokenize = tokenize,lower=True, unk_token=None)\n",
    "LABEL = Field(sequential=False, use_vocab=False,is_target  = True, unk_token=None)\n",
    "\n",
    "\n",
    "# In[206]:\n",
    "\n",
    "\n",
    "tv_datafields = [ (\"text\", TEXT), (\"label\", LABEL)]\n",
    "train, dev,test = TabularDataset.splits(\n",
    "               path=dataset_path, # the root directory where the data lies\n",
    "               train='train.csv', validation=\"valid.csv\", test = \"test.csv\",\n",
    "               format='csv', \n",
    "\n",
    "\n",
    "               skip_header=False, # if your csv header has a header, make sure to pass this to ensure it doesn't get proceesed as data!\n",
    "               fields=tv_datafields)\n",
    "\n",
    "\n",
    "\n",
    "train_iter, dev_iter, test_iter = data.BucketIterator.splits(\n",
    "    (train, dev, test), batch_size=this_batch_size, device=torch.device(gpu_device),\n",
    "    sort_key=lambda x: len(x.text), # the BucketIterator needs to be told what function it should use to group the data.\n",
    "             sort_within_batch=False,\n",
    "    shuffle =True,\n",
    "             repeat=False) # we pass repeat=False because we want to wrap this Iterator layer.,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[211]:\n",
    "\n",
    "\n",
    "TEXT.build_vocab(train, dev, test)\n",
    "LABEL.build_vocab(train, dev, test)\n",
    "\n",
    "\n",
    "# In[212]:\n",
    "\n",
    "\n",
    "TEXT.vocab.vectors = torch.eye(len(TEXT.vocab))\n",
    "\n",
    "\n",
    "# In[213]:\n",
    "\n",
    "\n",
    "class my_config:\n",
    "    d_hidden = 4\n",
    "    n_embed =len(TEXT.vocab)\n",
    "    d_embed =len(TEXT.vocab) \n",
    "    d_out =len(LABEL.vocab)\n",
    "    batch_size =10\n",
    "\n",
    "\n",
    "# In[252]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, batch in enumerate(train_iter):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMSentiment(\n",
       "  (embed): Embedding(7, 7)\n",
       "  (lstm): LSTM(7, 4)\n",
       "  (hidden_to_label): Linear(in_features=4, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LSTMSentiment(my_config)\n",
    "model.embed.weight.data = TEXT.vocab.vectors\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad>', 'e', 'f', 'c', 'd', 'a', 'b']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3591, device='cuda:0', grad_fn=<NegBackward>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_in_relevant(batch, start, stop,  class_rules,dim =0):\n",
    "\n",
    "    #XXX only for current model where relevant bigger five\n",
    "    rel_digits = ((batch.label ==class_rules[0][0])[None, :] *(batch.text ==class_rules[0][1])) + (batch.label ==class_rules[1][0])[None, :] *(batch.text ==class_rules[1][1])\n",
    "    relevant = rel_digits[start:stop].sum(dim=0)\n",
    "    irrelevant = rel_digits.sum(dim=0) - relevant\n",
    "    test_out = torch.cat((relevant[:, None], irrelevant[:, None]), 1)\n",
    "    test_out = test_out/ test_out.sum(dim=1)[:, None]\n",
    "    return test_out\n",
    "    \n",
    "\n",
    "def cd_penalty_for_one(batch, model1, start, stop,class_rules):\n",
    "   # get output\n",
    "    model1_output = cd_batch_text(batch, model1, start, stop)\n",
    "    # only use the correct class\n",
    "    correct_idx = (batch.label, torch.arange(batch.label.shape[0]))\n",
    "    model1_softmax = softmax_out((model1_output[0][correct_idx],model1_output[1][correct_idx]))\n",
    "    model2_softmax = is_in_relevant(batch, start, stop,class_rules).cuda().float()\n",
    "    \n",
    "   \n",
    "    output = -(torch.log(model1_softmax)*model2_softmax).sum() \n",
    " \n",
    "    return output\n",
    "cd_penalty_for_one(batch, model, 0, 1, ((1, 6), (0, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        [0, 1]], device='cuda:0')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_in_relevant(batch, 0, 1, ((1, 6), (0, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4, 2, 3, 6, 2],\n",
       "        [6, 2, 4, 2, 5],\n",
       "        [1, 2, 5, 2, 1],\n",
       "        [2, 1, 4, 2, 2],\n",
       "        [5, 5, 3, 3, 3]], device='cuda:0')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABEL.vocab.stoi['o']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['0', '1', 'o', 'ff'])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABEL.vocab.stoi.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(LABEL.vocab.itos[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 359.61it/s]\n"
     ]
    }
   ],
   "source": [
    "results_folder = \"../results\"\n",
    "fnames = sorted([oj(results_folder, fname) for fname in os.listdir(results_folder)]) # filenames in the directory\n",
    "results_list = [pd.Series(pkl.load(open(fname, \"rb\"))) for fname in tqdm(fnames) ] \n",
    "results = pd.concat(results_list, axis=1).T.infer_objects() # pandas dataframe w/ hyperparams and weights stored\n",
    "\n",
    "results[\"final_acc\"] = [max(x) for x in results[\"acc_test\"]]\n",
    "results[\"min_test_loss\"] = [min(x) for x in results[\"losses_test\"]]\n",
    "results[\"num_epochs\"] = [len(x) for x in results[\"losses_test\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function torchtext.vocab._default_unk_index()>, {'0': 0, '1': 1})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABEL.vocab.stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 0, 0, 0], device='cuda:0')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6, 2, 1, 4, 3],\n",
       "        [4, 3, 5, 5, 3],\n",
       "        [3, 5, 2, 1, 1],\n",
       "        [3, 2, 3, 3, 5]], device='cuda:0')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMSentiment(\n",
       "  (embed): Embedding(7, 7)\n",
       "  (lstm): LSTM(7, 4)\n",
       "  (hidden_to_label): Linear(in_features=4, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gpu_usage)",
   "language": "python",
   "name": "gpu_usage"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
